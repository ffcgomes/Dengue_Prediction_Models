## Entrada de Dados

```{r}
#Input Data
train<-read.csv('Sobral_Train.csv')
test<-read.csv('Sobral_Test.csv')
expl<-read.csv('Sobral_Train_Label.csv')
#combine training data set and its total cases
train$Infectados<-expl$Infectados
head(train)
#head(test)
```
I know some people would use ti Weekofyear' is pretty much the same thing, so delete week_start_date keep Weekofyear for convenience.
```{r}
#See how many percent of NAs in each columns in training and testing data set
colSums(is.na(train))/dim(train)[1]*100
```
```{r}
colSums(is.na(test))/dim(test)[1]*100
```
```{r}
#Delete all of NAs in training data set
```


```{r}
train<-na.omit(train)
```
```{r}
####Replace NA with the most last value in test dataset
library(zoo)
test$Precipitacao<-na.locf(test$Precipitacao)
test$TempMaxima<-na.locf(test$TempMaxima)
test$TempMinima<-na.locf(test$TempMinima)
test$Insolacao<-na.locf(test$Insolacao)
test$Evaporacao_Piche<-na.locf(test$Evaporacao_Piche)
test$Temp_Comp_Med<-na.locf(test$Temp_Comp_Med)
test$Umidade_Rel_Med<-na.locf(test$Umidade_Rel_Med)
test$Vel_Vento_Med<-na.locf(test$Vel_Vento_Med)
```

```{r}
#Check Whether there is any NA in each column
colSums(is.na(test))
```

```{r}
str(test)
```

```{r}
############Delete Identification Columns in Training data set
#train$city<-as.factor(train$city)
train$city=NULL
train$year<-NULL
train$weekofyear<-as.factor(train$weekofyear)
train$week_start_date<-NULL

############Delete Identification Columns in Testing data set
#test$city<-as.factor(test$city)
test$city=NULL
test$year<-NULL
test$weekofyear<-as.factor(test$weekofyear)
test$week_start_date<-NULL
```

Create Dummy Variables for the factors

```{r}
####Create Dummy Variables
library(dummies)
train<-dummy.data.frame(train)
test<-dummy.data.frame(test)
```
Model will be used includes Linear Regression, SVM, Random Forest
```{r}
####Random Split as training and validation
random<-sample(1:dim(train)[1])
pct2o3<-floor(dim(train)[1]*2/3)
train_df<-train[random[1:pct2o3],]
vali_df<-train[random[(pct2o3+1):dim(train)[1]],]
```


Linear Model
```{r}
####Linear Model
lm1<-lm(data = train_df,Infectados~.)
summary(lm1)
```
P-value larger than 0.05 would be considered as insignificant. Remove all of that from the model and re-fit it.

```{r}
#lm2<-lm(data= train_df,Infectados~weekofyear36)
#summary(lm2)
```
R-squared is really small means that Linear Regression Model may not be the best fit in this case, but still can be a baseline prediction model

```{r}
#Make prediction on validation dataset and calculate MAE
lmPred<-predict(lm1,vali_df)
MAE_lm<-mean(abs(lmPred-vali_df$Infectados))
cat('MAE_lm:',MAE_lm)
```
Support Vector Machine
```{r}
#Build SVM model, try different parameter of C
library(kernlab)
svmOutput1<-ksvm(Infectados~.,data=train_df, kernel="rbfdot",kpar="automatic",C=0.6,cross=3,prob.model=TRUE)
svmOutput2<-ksvm(Infectados~.,data=train_df, kernel="rbfdot",kpar="automatic",C=1,cross=3,prob.model=TRUE)
svmOutput3<-ksvm(Infectados~.,data=train_df, kernel="rbfdot",kpar="automatic",C=1.5,cross=3,prob.model=TRUE)
```

Compare results of three SVM model according to 3-fold cross validation error

```{r}
svmOutput1
```

```{r}
svmOutput3
```

As svm3 model has the lowest cross validation error, it is chosed to make further prediction on validation dataset
```{r}
#Make prediction on validation dataset
svmPred<-predict(svmOutput3,vali_df,type="votes")
MAE_svm<-mean(abs(svmPred-vali_df$Infectados))
MAE_svm
```
Random ForestÂ¶
```{r}
#Build the model
require(randomForest)
require(MASS)
rf1<-randomForest(train_df$Infectados~.,data=train_df,ntree=50,cross=3)
rf2<-randomForest(train_df$Infectados~.,data=train_df,ntree=250,cross=3)
rf3<-randomForest(train_df$Infectados~.,data=train_df,ntree=500,cross=3)
```
```{r}
rf1
```

```{r}
rf2
```

```{r}
rf3
```
According to the lowest cross-validation error, rf2 is chosen
```{r}
rfPred<-predict(rf2,vali_df)
MAE_rf<-mean(abs(rfPred-vali_df$Infectados))
MAE_rf
```
SummaryÂ¶
Linear Regression Model, SVM and Random Forest are tried to make prediction, according to the validation error, SVM2 is chosed.

There is potential improvement in each model by adjusting different parameters more.
As for if year should be considered into the model, the best way to say is to try!
